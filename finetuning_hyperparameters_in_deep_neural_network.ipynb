{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.3.0\n",
      "Keras Version:  2.4.0\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_jl_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\sample_submission.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\test.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\train.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "#import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import seed\n",
    "seed(1)\n",
    "seed = 43\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import image\n",
    "from tensorflow import core\n",
    "from tensorflow.keras import layers\n",
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "print(\"Keras Version: \",keras.__version__)\n",
    "\n",
    "\n",
    "kaggle = 0 # Kaggle path active = 1\n",
    "\n",
    "# change your local path here\n",
    "if kaggle == 1 :\n",
    "    MNIST_PATH= '../input/digit-recognizer'\n",
    "else:\n",
    "    MNIST_PATH= '../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer'\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(MNIST_PATH): \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - MNIST Training Competition\n",
    "This notebook is a fork of my previous developed notebook for digit recognition. Therefore you will find some parts that look common the the notebook <a href=\"https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\">Digit Recognition with a Deep Neural Network</a> and some parts that are completely different.\n",
    "\n",
    "With this I want to take a deeper look in some parts of finetuning hyperparameters. The following list shows some of the finetuning parameters which I will take a look into, one or two ore more ... :\n",
    "- Dwindling / Exploding Gradients\n",
    "    - <b>Initializing the Weights</b>\n",
    "    - <b>Batchnormalization</b>\n",
    "    - <s>Gradient Clipping</s>\n",
    "    - <b>Saturated Activataion Functions</b>\n",
    "- Optimizers\n",
    "    - <s>Momentum Optimizers</s>\n",
    "    - <s>Nesterov</s>\n",
    "    - <s>AdaGrad</s>\n",
    "    - <s>RMSProp</s>\n",
    "    - <b>Adam - Optimizer</b>\n",
    "    - <s>Scheduling Learnrate</s>\n",
    "- Regulations\n",
    "    - <s>Drop-Outs</s>\n",
    "    - <b>l1 / l2 - Regulations</b>\n",
    "    - <s>Monte-Carlo Drop-out ???</s>\n",
    "    - <s>Max Norm Regulations ????</s>\n",
    "\n",
    "Not part of this notebook will be the use of pretrained neural networks (Transferlearning). I just want to list this here for the sake of completeness.\n",
    "\n",
    "Link to the data topic: https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "As in the previous notebooks I will use Tensorflow with Keras. I already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform/prepare the data rightaway.\n",
    "\n",
    "## Notebook Versions with Different Hyperparameter Configurations\n",
    "As described in the part above, I used/tested different hyperparameter settings to get a little bit closer to its effects on the neural network and the network's results. I know that there are parameters that effect other parameters when they have changed (and therefore should have been changed as well), however in these cases I just tried a little bit around. Sometimes I kept one or two parameters together, which should be together (e.g. kernel initializer \"lecun\" and activation function \"selu\") and sometimes not. The main purpose here was to use them and see the results.\n",
    "\n",
    "Therefore on Kaggle you can look in the different versions of this notebook if you are interested. In the following I will list some versions with the used hyperparameter config in it:\n",
    "- Version 7 and 6:\n",
    "    - Activation Function - \"relu\"\n",
    "    - Initializing Weights - \"He Normalization\"\n",
    "    - Batchnormalization\n",
    "- Version 9:\n",
    "    - Activation Function - \"selu\"\n",
    "    - Initializing Weights - \"LeCun Normal\"\n",
    "- Version 12 and 14:\n",
    "    - Regularisation with L1 and L2\n",
    "- Version 15:\n",
    "    - Activation Function - \"relu\"\n",
    "    - Initializing Weights - \"He Normalization\"\n",
    "    - Batchnormalization\n",
    "    - Optimizer - \"Adam\"\n",
    "\n",
    "The current best run was based on the Version 7 with an accuracy of 0.97657 on the kaggle competition \"Digit Recognzier\"\n",
    "\n",
    "\n",
    "## My other Projects\n",
    "If you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:\n",
    "- Digit Recognition with a Deep Neural Network: https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\n",
    "- Another MNIST Try: https://www.kaggle.com/skiplik/another-mnist-try\n",
    "- First NN by Detecting Handwritten Characters: https://www.kaggle.com/skiplik/first-nn-by-detecting-handwritten-characters\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path and file\n",
    "CSV_FILE_TRAIN='train.csv'\n",
    "CSV_FILE_TEST='test.csv'\n",
    "\n",
    "def load_mnist_data(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_mnist_data_manuel(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    csv_file = open(csv_path, 'r')\n",
    "    csv_data = csv_file.readlines()\n",
    "    csv_file.close()\n",
    "    return csv_data\n",
    "\n",
    "def split_train_val(data, val_ratio):\n",
    "    return \n",
    "    \n",
    "\n",
    "train = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\n",
    "test = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label'].copy()\n",
    "X = train.drop(['label'], axis=1)\n",
    "\n",
    "# competition dataset\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Features:  (42000, 784)\n",
      "Shape of the Labels:  (42000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Features: \",X.shape)\n",
    "print(\"Shape of the Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Value Count\n",
    "Visualizing the label distribution of the full train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.20\n",
    "                                                  , stratify=y\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the equally splitted train- and val-sets based on the given label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Set Distribution\n",
      "1    0.111518\n",
      "7    0.104792\n",
      "3    0.103601\n",
      "9    0.099702\n",
      "2    0.099464\n",
      "6    0.098512\n",
      "0    0.098363\n",
      "4    0.096964\n",
      "8    0.096726\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Val - Set Distribution\n",
      "1    0.111548\n",
      "7    0.104762\n",
      "3    0.103571\n",
      "9    0.099762\n",
      "2    0.099405\n",
      "0    0.098452\n",
      "6    0.098452\n",
      "4    0.096905\n",
      "8    0.096786\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train - Set Distribution\")\n",
    "print(y_train.value_counts() / y_train.value_counts().sum() )\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print(\"Val - Set Distribution\")\n",
    "print(y_val.value_counts() / y_val.value_counts().sum() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (42000, 784)\n",
      "X_train:  (33600, 784)\n",
      "X_val:  (8400, 784)\n",
      "y_train:  (33600,)\n",
      "y_val:  (8400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \", X.shape)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_val: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Transforming Piplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('normalizer', Normalizer())\n",
    "    ('std_scalar',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation with Tensorflow Data Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 85 // 100       # croping to 90% of the initial picture \n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 1])\n",
    "\n",
    "\n",
    "def crop_flip_resize(image, label, flipping = True):\n",
    "    if flipping == True:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = random_crop(image)\n",
    "\n",
    "    ## final solution\n",
    "    resized_image = tf.image.resize(cropped_image, [28,28])\n",
    "    final_image = resized_image\n",
    "    #final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_val = X_val.values.reshape(X_val.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train_crop = X_train.copy()\n",
    "X_val_crop = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensorbased dataset \n",
    "\n",
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "training_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train_crop, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val_crop, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function random_crop at 0x000001A78B1F1C10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function random_crop at 0x000001A78B1F1C10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# resizing, croping images via self build function\n",
    "training_crop_dataset = training_crop_dataset.map(partial(crop_flip_resize, flipping=False))\n",
    "val_crop_dataset = val_crop_dataset.map(partial(crop_flip_resize, flipping=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQOklEQVR4nO3df5BV9XnH8c+zyy4ILL8EVqKrAtLG30S3aGtiTI0OMU7RSePEP6ydcYZMR1szk5nWSf+InfYPpo0/8ofNlEQqtomaNhppaxMpsYO2U+KqFDCoIKKurCyCyG929+7TP/bY2eCe5673nvsDv+/XzM69e5579jxc9rPn3vs953zN3QXgk6+l0Q0AqA/CDiSCsAOJIOxAIgg7kIgJ9dxYu030SZpSz00mb+D0+PmeO+ODuN46ENb7S+1hfe+703NrE/YfDdf10nBYx0cd02EN+HEbq1ZV2M1sqaTvSmqV9AN3XxE9fpKm6DK7uppNYiw25v+tJOmNP7k8XPXO3/vXsH77jLfD+gP7u8L6wyuuz62d+tOXw3VLBw6EdXzUBl+XW6v4ZbyZtUp6QNKXJJ0n6WYzO6/Snwegtqp5z75E0nZ33+HuA5IelbSsmLYAFK2asJ8uafRrvN5s2a8xs+Vm1mNmPYM6XsXmAFSjmrCP9UbxI8feuvtKd+929+42TaxicwCqUU3YeyWN/nTmDEm7qmsHQK1UE/bnJS0ys/lm1i7pa5LWFNMWgKJVPPTm7kNmdoekn2tk6G2Vu8djKahIS0dHWP/gy+fn1j531eZw3WVTt4b1v9l3cVj/wRPXhvVzfvFWbm3o0OFwXRSrqnF2d39K0lMF9QKghjhcFkgEYQcSQdiBRBB2IBGEHUgEYQcSUdfz2VGZlqnxOel91wzl1lbMfS5c94wJU8P6s3sXhfXTNpTC+lDvO2Ed9cOeHUgEYQcSQdiBRBB2IBGEHUgEYQcSwdBbE7CJ8RV8hrrmhPWbLunJrS2acChct7+Uf2VaSXptd7ztBa+/H9bjgTnUE3t2IBGEHUgEYQcSQdiBRBB2IBGEHUgEYQcSwTh7E2idfWpY77s0Pg11+az801jntk4O1/2nQ/G2h7fH2x7e/mpYR/Ngzw4kgrADiSDsQCIIO5AIwg4kgrADiSDsQCIYZ28CA/PnhnVfGp8z3tma/9/YavHf83/uvzSsT98eluWDA/ED0DSqCruZ7ZR0UCPXKBhy9+4imgJQvCL27F9w9/cK+DkAaoj37EAiqg27S3razF4ws+VjPcDMlptZj5n1DOp4lZsDUKlqX8Zf4e67zGyupLVm9oq7rx/9AHdfKWmlJE2zWV7l9gBUqKo9u7vvym77JT0haUkRTQEoXsVhN7MpZtbx4X1J10raUlRjAIpVzcv4TklPmNmHP+dH7v6zQrpCYQY9vnL7Szu7wvrCV48W2Q4aqOKwu/sOSRcX2AuAGmLoDUgEYQcSQdiBRBB2IBGEHUgEp7h+wh3x+BTUtjcmhfUJv3olrDMl88mDPTuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4lgnP0TbtjjiwO1HbSwXno/vow1Th7s2YFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSATj7GharXPmhHXrmBLWhztOya0NTStzHv+BY2G9Zfe+sF56f39Y9+P1nwqNPTuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4lgnD118ensksUPsPb2sN4ybVr+ulPyx8Elae9nPxXWD3bF+6ojXUO5tTlnxefpv/fGrLA+66UZYX32pkNhvXV7b26tVtcQKLtnN7NVZtZvZltGLZtlZmvNbFt2O7Mm3QEozHhexj8kaekJy+6StM7dF0lal30PoImVDbu7r5d04rGByyStzu6vlnRDsW0BKFqlH9B1unufJGW3c/MeaGbLzazHzHoGVf/jgQGMqPmn8e6+0t273b27TRNrvTkAOSoN+24zmydJ2W1/cS0BqIVKw75G0q3Z/VslPVlMOwBqpew4u5k9IukqSbPNrFfStyWtkPRjM7tN0luSvlrLJlE7w2V+A1qn54+TS9LwOV1hffvvd+TWzrnszXDdu7v+Pqxf3L43rE+y/H1ZW1CTpMGLhsP6u9eHZT2w5wthff2jl+bW5t23If7hw6W4nqNs2N395pzS1RVtEUBDcLgskAjCDiSCsAOJIOxAIgg7kAhOcf2Em9YSXzJ54uXx8NUrneeG9UUX5J+qKUnfOfPx3Nqn2/eE657R2hbWNw3Ep8juHJydW7ts0tvhuvPbpob1aS3x0NxdnevC+lMXnZ9bm3B2PJw5tGNnWM/Dnh1IBGEHEkHYgUQQdiARhB1IBGEHEkHYgUQwzt4E2vYeDuuHt+aPF0vSsUvyT3mc2hL/Pf/L8+JLEez5jfgU18+dsiOsnz1hcm7t+ePxMQBfefGWsF7qmRHW24KrOR8+Mx4nX7g4Pn7gb895NF6/zDj9vM79ubXjZ8aXsW5lnB1AhLADiSDsQCIIO5AIwg4kgrADiSDsQCIYZ28GffEcG6f9TzxJ7s9vPDO3dv2UeLz4y/nD4JljYfWD4daw/i9H8sfp/7TnK+G6pz0WzyA09blXw7ofOZpfPCf/OZOkHUfic8pfPyv+P1nYNhjWz+rIn5Z5+9md4bqVTpnMnh1IBGEHEkHYgUQQdiARhB1IBGEHEkHYgUQwzt4ESvs/COsdG98N6/e+lj+h7oUXPhyue1F7WC6rdyiu3//GF3Nrn/rHeOOT/3tbWC+9nz9WLUk2If/Xu+XoQLhuud1gq8Xnw5czZUL+9kvx4QUVK7tnN7NVZtZvZltGLbvbzN4xs43Z13W1aQ9AUcbzMv4hSUvHWH6fuy/Ovp4qti0ARSsbdndfL2lfHXoBUEPVfEB3h5ltyl7m5x6ua2bLzazHzHoGdbyKzQGoRqVh/56khZIWS+qTdE/eA919pbt3u3t3m2r0yQOAsioKu7vvdveSuw9L+r6kJcW2BaBoFYXdzOaN+vZGSVvyHgugOZQdZzezRyRdJWm2mfVK+rakq8xssSSXtFPS12vXIvxQfF35o788I7e27Tfnhute1H6gop4+dMzj89n3HJySWzvtaDxWbZPit32tM6aHdc3Ov/763svjc8YvvDIe418wIT42YtDjueOffXNB/s9e/164bv4sAbGyYXf3m8dY/GCF2wPQIBwuCySCsAOJIOxAIgg7kAjCDiSCU1xPBgPxZYkn7/Lc2t5SPHWwVN3Q20Xt8dDbmu6/y609dv+l4bqvHI6Hx44MxafInjZpV27tD6b9Z7ju706OL8F9akt8De67dsf/trZfduTWvHdnuG6l2LMDiSDsQCIIO5AIwg4kgrADiSDsQCIIO5AIxtlPAsNHjoT1zmf6cmv3XJt/KWdJ+vRvxZeavnJSWFabxePsC9vyx/n/aOZL4bqHZ8SnwJa7mHNbUOtoiX/1j+UfuiBJ+uNdvxPWn33skrDetWZ3bq106FC88QqxZwcSQdiBRBB2IBGEHUgEYQcSQdiBRBB2IBGMs58EfCieF3lox87c2vR//+1w3fvnXRPWz5v/07A+uzX/UtHlzGyNzwnPnVMs8/LA0bD+X0cX5tZeOHhWuO7Tm88P6zOfj0bxpa5f5I+jS1LptdfDei2wZwcSQdiBRBB2IBGEHUgEYQcSQdiBRBB2IBGMs3/CzfmPt8L6K12Lwvpf3fD5sP4Xnc+G9ekt+VMXvzEYn7f93feuCuv/9uoFYd3ezN/25D4L1z133d6w7q+/GdZLx46F9UYou2c3sy4ze8bMtprZy2Z2Z7Z8lpmtNbNt2W25YyAANNB4XsYPSfqmu58r6XJJt5vZeZLukrTO3RdJWpd9D6BJlQ27u/e5+4vZ/YOStko6XdIySauzh62WdEONegRQgI/1AZ2ZnS3pM5I2SOp09z5p5A+CpLk56yw3sx4z6xnU8SrbBVCpcYfdzKZK+omkb7j7uGcDdPeV7t7t7t1tmlhJjwAKMK6wm1mbRoL+Q3d/PFu828zmZfV5kvpr0yKAIpQdejMzk/SgpK3ufu+o0hpJt0pakd0+WZMOUZWh3nfC+vyH4iGodceXhPXhm+L1F5yyJ7f2s93xaaT7V3WF9UVrd4T1Un/+tuXxtaJLYfXkNJ5x9isk3SJps5ltzJZ9SyMh/7GZ3SbpLUlfrUmHAApRNuzu/pykvD/fVxfbDoBa4XBZIBGEHUgEYQcSQdiBRBB2IBHmZcYbizTNZvllxgf4J5WWeEpma43rIY8nXfZSmdHuOv7uniw2+Dod8H1jjp6xZwcSQdiBRBB2IBGEHUgEYQcSQdiBRBB2IBFcShqx4Xis28vU0TzYswOJIOxAIgg7kAjCDiSCsAOJIOxAIgg7kAjCDiSCsAOJIOxAIgg7kAjCDiSCsAOJIOxAIgg7kIiyYTezLjN7xsy2mtnLZnZntvxuM3vHzDZmX9fVvl0AlRrPxSuGJH3T3V80sw5JL5jZ2qx2n7t/p3btASjKeOZn75PUl90/aGZbJZ1e68YAFOtjvWc3s7MlfUbShmzRHWa2ycxWmdnMnHWWm1mPmfUM6nh13QKo2LjDbmZTJf1E0jfc/YCk70laKGmxRvb894y1nruvdPdud+9u08TqOwZQkXGF3czaNBL0H7r745Lk7rvdveTuw5K+L2lJ7doEUK3xfBpvkh6UtNXd7x21fN6oh90oaUvx7QEoyng+jb9C0i2SNpvZxmzZtyTdbGaLJbmknZK+XoP+ABRkPJ/GPydprPmenyq+HQC1whF0QCIIO5AIwg4kgrADiSDsQCIIO5AIwg4kgrADiSDsQCIIO5AIwg4kgrADiSDsQCIIO5AIc/f6bcxsj6Q3Ry2aLem9ujXw8TRrb83al0RvlSqyt7Pcfc5YhbqG/SMbN+tx9+6GNRBo1t6atS+J3ipVr954GQ8kgrADiWh02Fc2ePuRZu2tWfuS6K1Sdemtoe/ZAdRPo/fsAOqEsAOJaEjYzWypmb1qZtvN7K5G9JDHzHaa2eZsGuqeBveyysz6zWzLqGWzzGytmW3LbsecY69BvTXFNN7BNOMNfe4aPf153d+zm1mrpNckXSOpV9Lzkm5291/VtZEcZrZTUre7N/wADDO7UtIhSQ+7+wXZsr+WtM/dV2R/KGe6+581SW93SzrU6Gm8s9mK5o2eZlzSDZL+UA187oK+blIdnrdG7NmXSNru7jvcfUDSo5KWNaCPpufu6yXtO2HxMkmrs/urNfLLUnc5vTUFd+9z9xez+wclfTjNeEOfu6CvumhE2E+X9Pao73vVXPO9u6SnzewFM1ve6GbG0OnufdLIL4+kuQ3u50Rlp/GupxOmGW+a566S6c+r1YiwjzWVVDON/13h7pdI+pKk27OXqxifcU3jXS9jTDPeFCqd/rxajQh7r6SuUd+fIWlXA/oYk7vvym77JT2h5puKeveHM+hmt/0N7uf/NdM03mNNM64meO4aOf15I8L+vKRFZjbfzNolfU3Smgb08RFmNiX74ERmNkXStWq+qajXSLo1u3+rpCcb2MuvaZZpvPOmGVeDn7uGT3/u7nX/knSdRj6Rf13Snzeih5y+Fkj63+zr5Ub3JukRjbysG9TIK6LbJJ0qaZ2kbdntrCbq7R8kbZa0SSPBmteg3j6rkbeGmyRtzL6ua/RzF/RVl+eNw2WBRHAEHZAIwg4kgrADiSDsQCIIO5AIwg4kgrADifg/B8K3STGfyngAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing a croped, flipped, resized image from new dataset.\n",
    "for X_values, y_values in training_crop_dataset.take(1):\n",
    "    for index in range(1):\n",
    "        plt.imshow(X_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate the two datasets\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset_all length:  67200\n",
      "val_dataset_all length:  16800\n"
     ]
    }
   ],
   "source": [
    "print(\"training_dataset_all length: \", len(list(training_dataset_all)))\n",
    "print(\"val_dataset_all length: \", len(list(val_dataset_all)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffeling and batching data\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "train_ds = training_dataset_all.shuffle(10000).batch(32).prefetch(1)\n",
    "val_ds = val_dataset_all.shuffle(8000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Model Visualization with Tensorboard (not for Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative root_logdir:  ../../tensorboard-logs\n"
     ]
    }
   ],
   "source": [
    "root_logdir = \"../../tensorboard-logs\"\n",
    "\n",
    "print(\"Relative root_logdir: \",root_logdir)\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run logdir for Tensorboard:  ../../tensorboard-logs\\run_2021_10_29-15_44_15\n"
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "print(\"Current run logdir for Tensorboard: \", run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../tensorboard-logs\\\\run_2021_10_29-15_44_15'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks for Tensorboard\n",
    "With Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "\n",
    "input_shape=[784]\n",
    "input_shape_notFlattened=[28,28]\n",
    "\n",
    "\n",
    "learning_rt = 1e-03 \n",
    "activation_fn = \"relu\"\n",
    "initializer = \"he_normal\"\n",
    "regularizer =  None #keras.regularizers.l2(0.01)\n",
    "\n",
    "# Model building\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=input_shape_notFlattened))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer)) ## add  kernel_regularizer=keras.regularizers.l2(0.01)) ???\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\"))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rt)\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 699,146\n",
      "Trainable params: 694,578\n",
      "Non-trainable params: 4,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_train_model.h5\", save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "   2/2100 [..............................] - ETA: 27:44 - loss: 2.7362 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0060s vs `on_train_batch_end` time: 1.5805s). Check your callbacks.\n",
      "2100/2100 [==============================] - 12s 6ms/step - loss: 0.4569 - accuracy: 0.8576 - val_loss: 0.2474 - val_accuracy: 0.9230\n",
      "Epoch 2/200\n",
      "2100/2100 [==============================] - 11s 5ms/step - loss: 0.2602 - accuracy: 0.9186 - val_loss: 0.2063 - val_accuracy: 0.9343\n",
      "Epoch 3/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.2050 - accuracy: 0.9364 - val_loss: 0.1598 - val_accuracy: 0.9527\n",
      "Epoch 4/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.1768 - accuracy: 0.9446 - val_loss: 0.1508 - val_accuracy: 0.9554\n",
      "Epoch 5/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.1465 - accuracy: 0.9538 - val_loss: 0.1339 - val_accuracy: 0.9592\n",
      "Epoch 6/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.1319 - accuracy: 0.9584 - val_loss: 0.1249 - val_accuracy: 0.9628\n",
      "Epoch 7/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.1169 - accuracy: 0.9633 - val_loss: 0.1153 - val_accuracy: 0.9663\n",
      "Epoch 8/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.1047 - accuracy: 0.9667 - val_loss: 0.1079 - val_accuracy: 0.9683\n",
      "Epoch 9/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0949 - accuracy: 0.9694 - val_loss: 0.1099 - val_accuracy: 0.9680\n",
      "Epoch 10/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0901 - accuracy: 0.9713 - val_loss: 0.1016 - val_accuracy: 0.9691\n",
      "Epoch 11/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0818 - accuracy: 0.9738 - val_loss: 0.1015 - val_accuracy: 0.9702\n",
      "Epoch 12/200\n",
      "2100/2100 [==============================] - 11s 5ms/step - loss: 0.0771 - accuracy: 0.9751 - val_loss: 0.1019 - val_accuracy: 0.9716\n",
      "Epoch 13/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0701 - accuracy: 0.9771 - val_loss: 0.1108 - val_accuracy: 0.9684\n",
      "Epoch 14/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0670 - accuracy: 0.9790 - val_loss: 0.0901 - val_accuracy: 0.9742\n",
      "Epoch 15/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0623 - accuracy: 0.9798 - val_loss: 0.0994 - val_accuracy: 0.9730\n",
      "Epoch 16/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0575 - accuracy: 0.9812 - val_loss: 0.1010 - val_accuracy: 0.9729\n",
      "Epoch 17/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0563 - accuracy: 0.9817 - val_loss: 0.1035 - val_accuracy: 0.9735\n",
      "Epoch 18/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0536 - accuracy: 0.9821 - val_loss: 0.0922 - val_accuracy: 0.9739\n",
      "Epoch 19/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0510 - accuracy: 0.9837 - val_loss: 0.0960 - val_accuracy: 0.9749\n",
      "Epoch 20/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0472 - accuracy: 0.9849 - val_loss: 0.0966 - val_accuracy: 0.9755\n",
      "Epoch 21/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0482 - accuracy: 0.9848 - val_loss: 0.0935 - val_accuracy: 0.9764\n",
      "Epoch 22/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0435 - accuracy: 0.9854 - val_loss: 0.0925 - val_accuracy: 0.9749\n",
      "Epoch 23/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0435 - accuracy: 0.9863 - val_loss: 0.0890 - val_accuracy: 0.9759\n",
      "Epoch 24/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0413 - accuracy: 0.9865 - val_loss: 0.1050 - val_accuracy: 0.9752\n",
      "Epoch 25/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0408 - accuracy: 0.9870 - val_loss: 0.0891 - val_accuracy: 0.9765\n",
      "Epoch 26/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0392 - accuracy: 0.9878 - val_loss: 0.0947 - val_accuracy: 0.9753\n",
      "Epoch 27/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0396 - accuracy: 0.9873 - val_loss: 0.1011 - val_accuracy: 0.9768\n",
      "Epoch 28/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0356 - accuracy: 0.9885 - val_loss: 0.0917 - val_accuracy: 0.9771\n",
      "Epoch 29/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0360 - accuracy: 0.9883 - val_loss: 0.0888 - val_accuracy: 0.9765\n",
      "Epoch 30/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0346 - accuracy: 0.9885 - val_loss: 0.0916 - val_accuracy: 0.9769\n",
      "Epoch 31/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0348 - accuracy: 0.9889 - val_loss: 0.0962 - val_accuracy: 0.9771\n",
      "Epoch 32/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0323 - accuracy: 0.9895 - val_loss: 0.0937 - val_accuracy: 0.9766\n",
      "Epoch 33/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0320 - accuracy: 0.9897 - val_loss: 0.0943 - val_accuracy: 0.9758\n",
      "Epoch 34/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0320 - accuracy: 0.9897 - val_loss: 0.0869 - val_accuracy: 0.9787\n",
      "Epoch 35/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0311 - accuracy: 0.9900 - val_loss: 0.0954 - val_accuracy: 0.9769\n",
      "Epoch 36/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0313 - accuracy: 0.9903 - val_loss: 0.0889 - val_accuracy: 0.9769\n",
      "Epoch 37/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0296 - accuracy: 0.9906 - val_loss: 0.0958 - val_accuracy: 0.9756\n",
      "Epoch 38/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0300 - accuracy: 0.9905 - val_loss: 0.0918 - val_accuracy: 0.9772\n",
      "Epoch 39/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.0998 - val_accuracy: 0.9771\n",
      "Epoch 40/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0292 - accuracy: 0.9903 - val_loss: 0.0982 - val_accuracy: 0.9779\n",
      "Epoch 41/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0264 - accuracy: 0.9915 - val_loss: 0.0920 - val_accuracy: 0.9780\n",
      "Epoch 42/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0284 - accuracy: 0.9913 - val_loss: 0.0930 - val_accuracy: 0.9779\n",
      "Epoch 43/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.0926 - val_accuracy: 0.9782\n",
      "Epoch 44/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0252 - accuracy: 0.9919 - val_loss: 0.0935 - val_accuracy: 0.9785\n",
      "Epoch 45/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0232 - accuracy: 0.9929 - val_loss: 0.0971 - val_accuracy: 0.9785\n",
      "Epoch 46/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0246 - accuracy: 0.9921 - val_loss: 0.0992 - val_accuracy: 0.9775\n",
      "Epoch 47/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0255 - accuracy: 0.9921 - val_loss: 0.0993 - val_accuracy: 0.9793\n",
      "Epoch 48/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0251 - accuracy: 0.9919 - val_loss: 0.1056 - val_accuracy: 0.9772\n",
      "Epoch 49/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0246 - accuracy: 0.9924 - val_loss: 0.0932 - val_accuracy: 0.9767\n",
      "Epoch 50/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 0.1005 - val_accuracy: 0.9773\n",
      "Epoch 51/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0224 - accuracy: 0.9928 - val_loss: 0.0964 - val_accuracy: 0.9786\n",
      "Epoch 52/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0222 - accuracy: 0.9926 - val_loss: 0.1013 - val_accuracy: 0.9766\n",
      "Epoch 53/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 0.0956 - val_accuracy: 0.9796\n",
      "Epoch 54/200\n",
      "2100/2100 [==============================] - 10s 5ms/step - loss: 0.0204 - accuracy: 0.9933 - val_loss: 0.1020 - val_accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=200, validation_data=(val_ds), callbacks=[checkpoint_cb, keras.callbacks.EarlyStopping(patience=20), tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with Full Dataset \n",
    "In this part I will train the model with the full dataset. This time I will use the discovered hyperparameters from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model building\n",
    "model_full = keras.models.Sequential()\n",
    "\n",
    "model_full.add(keras.layers.Flatten(input_shape=input_shape_notFlattened))\n",
    "model_full.add(keras.layers.BatchNormalization())\n",
    "model_full.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer)) ## add  kernel_regularizer=keras.regularizers.l2(0.01)) ???\n",
    "model_full.add(keras.layers.BatchNormalization())\n",
    "model_full.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n",
    "model_full.add(keras.layers.BatchNormalization())\n",
    "model_full.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n",
    "model_full.add(keras.layers.BatchNormalization())\n",
    "model_full.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n",
    "model_full.add(keras.layers.BatchNormalization())\n",
    "model_full.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n",
    "model_full.add(keras.layers.BatchNormalization())\n",
    "model_full.add(keras.layers.Dense(300, activation=activation_fn, kernel_initializer=initializer, kernel_regularizer= regularizer))\n",
    "\n",
    "model_full.add(keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\"))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=learning_rt)\n",
    "\n",
    "\n",
    "model_full.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 699,146\n",
      "Trainable params: 694,578\n",
      "Non-trainable params: 4,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new log dir for tensorboard\n",
    "tensorboard_cb_f = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "checkpoint_cb_f = keras.callbacks.ModelCheckpoint(\"my_modell_full.h5\", save_best_only=False, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing full features set (X) for the tensorflow data api\n",
    "\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)\n",
    "\n",
    "training_ds_all = training_dataset_all.concatenate(val_dataset_all)\n",
    "\n",
    "training_ds_all = training_ds_all.shuffle(20000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "   1/2625 [..............................] - ETA: 0s - loss: 3.2111 - accuracy: 0.0625WARNING:tensorflow:From D:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0055s vs `on_train_batch_end` time: 0.0385s). Check your callbacks.\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 1.4008 - accuracy: 0.5414\n",
      "Epoch 2/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.7951 - accuracy: 0.7428\n",
      "Epoch 3/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.6236 - accuracy: 0.7997\n",
      "Epoch 4/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.5300 - accuracy: 0.8311\n",
      "Epoch 5/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.4738 - accuracy: 0.8485\n",
      "Epoch 6/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.4314 - accuracy: 0.8623\n",
      "Epoch 7/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.3993 - accuracy: 0.8721\n",
      "Epoch 8/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.3725 - accuracy: 0.8806\n",
      "Epoch 9/60\n",
      "2625/2625 [==============================] - 11s 4ms/step - loss: 0.3523 - accuracy: 0.8868\n",
      "Epoch 10/60\n",
      "2625/2625 [==============================] - 11s 4ms/step - loss: 0.3305 - accuracy: 0.8931\n",
      "Epoch 11/60\n",
      "2625/2625 [==============================] - 11s 4ms/step - loss: 0.3184 - accuracy: 0.8981\n",
      "Epoch 12/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.3013 - accuracy: 0.9041\n",
      "Epoch 13/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.2900 - accuracy: 0.9066\n",
      "Epoch 14/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.2810 - accuracy: 0.9097\n",
      "Epoch 15/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.2703 - accuracy: 0.9137\n",
      "Epoch 16/60\n",
      "2625/2625 [==============================] - 11s 4ms/step - loss: 0.2615 - accuracy: 0.9165\n",
      "Epoch 17/60\n",
      "2625/2625 [==============================] - 11s 4ms/step - loss: 0.2507 - accuracy: 0.9198\n",
      "Epoch 18/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.2465 - accuracy: 0.9215\n",
      "Epoch 19/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.2406 - accuracy: 0.9232\n",
      "Epoch 20/60\n",
      "2625/2625 [==============================] - 11s 4ms/step - loss: 0.2335 - accuracy: 0.9255\n",
      "Epoch 21/60\n",
      "2625/2625 [==============================] - 11s 4ms/step - loss: 0.2272 - accuracy: 0.9284\n",
      "Epoch 22/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.2238 - accuracy: 0.9284\n",
      "Epoch 23/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.2159 - accuracy: 0.9305\n",
      "Epoch 24/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.2104 - accuracy: 0.9330\n",
      "Epoch 25/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.2080 - accuracy: 0.9338\n",
      "Epoch 26/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.2036 - accuracy: 0.9357\n",
      "Epoch 27/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1996 - accuracy: 0.9365\n",
      "Epoch 28/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1959 - accuracy: 0.9372\n",
      "Epoch 29/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1935 - accuracy: 0.9388\n",
      "Epoch 30/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1906 - accuracy: 0.9394\n",
      "Epoch 31/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1872 - accuracy: 0.9409\n",
      "Epoch 32/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1818 - accuracy: 0.9420\n",
      "Epoch 33/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1800 - accuracy: 0.9430\n",
      "Epoch 34/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1776 - accuracy: 0.9433\n",
      "Epoch 35/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1753 - accuracy: 0.9438\n",
      "Epoch 36/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1735 - accuracy: 0.9452\n",
      "Epoch 37/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1714 - accuracy: 0.9452\n",
      "Epoch 38/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1667 - accuracy: 0.9466\n",
      "Epoch 39/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1648 - accuracy: 0.9474\n",
      "Epoch 40/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1620 - accuracy: 0.9486\n",
      "Epoch 41/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1614 - accuracy: 0.9484\n",
      "Epoch 42/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1584 - accuracy: 0.9488\n",
      "Epoch 43/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1550 - accuracy: 0.9509\n",
      "Epoch 44/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1541 - accuracy: 0.9512\n",
      "Epoch 45/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1555 - accuracy: 0.9503\n",
      "Epoch 46/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1491 - accuracy: 0.9532\n",
      "Epoch 47/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1489 - accuracy: 0.9526\n",
      "Epoch 48/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1465 - accuracy: 0.9539\n",
      "Epoch 49/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1433 - accuracy: 0.9547\n",
      "Epoch 50/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1440 - accuracy: 0.9548\n",
      "Epoch 51/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1415 - accuracy: 0.9544\n",
      "Epoch 52/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1383 - accuracy: 0.9559\n",
      "Epoch 53/60\n",
      "2625/2625 [==============================] - 11s 4ms/step - loss: 0.1424 - accuracy: 0.9547\n",
      "Epoch 54/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1385 - accuracy: 0.9554\n",
      "Epoch 55/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1381 - accuracy: 0.9565\n",
      "Epoch 56/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1371 - accuracy: 0.9568\n",
      "Epoch 57/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1337 - accuracy: 0.9574\n",
      "Epoch 58/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1321 - accuracy: 0.9578\n",
      "Epoch 59/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1311 - accuracy: 0.9587\n",
      "Epoch 60/60\n",
      "2625/2625 [==============================] - 10s 4ms/step - loss: 0.1273 - accuracy: 0.9598\n"
     ]
    }
   ],
   "source": [
    "# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X \n",
    "history_full = model_full.fit(training_ds_all, epochs=60, callbacks=[tensorboard_cb_f, checkpoint_cb_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history_full.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Prediction of Unknown Data (Test Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peparing Test Data\n",
    "As well as previously done, we need to create a TF dataset of the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_test, tf.float32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (28, 28, 1), types: tf.float32>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Competition File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file = pd.DataFrame(columns=['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the image\n",
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch in test_ds.take(1):\n",
    "    for index in range(1):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propability of all lables for given pixels:  [2.4089815e-08 7.4411575e-05 9.9990892e-01 5.6008630e-06 4.7317304e-08\n",
      " 2.3272852e-07 7.5315036e-08 1.0460487e-05 1.8115448e-07 1.3897583e-09]\n"
     ]
    }
   ],
   "source": [
    "for element in test_ds.take(1):\n",
    "    print(\"Propability of all lables for given pixels: \", model_full.predict(test_ds.take(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Digit: \",np.argmax(model_full.predict(test_ds.take(1))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_full.predict(test_ds)                                                                           # predict the probability\n",
    "predictions = np.argmax(predictions, axis=1)                                                                        # getting the predicted digit numbers based ont the probability of every np element \n",
    "mnist_competition_file = pd.DataFrame(predictions)                                                                  # converting into df\n",
    "mnist_competition_file.index += 1                                                                                   # index should start at 1\n",
    "mnist_competition_file.reset_index(level=0, inplace=True)                                                           # make the index a column \n",
    "mnist_competition_file = mnist_competition_file.rename(columns={\"index\": \"ImageId\", 0: \"Label\"}, errors=\"raise\")    # renamen them according to the competition requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      9\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_competition_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.ImageId = mnist_competition_file.ImageId.astype(int)\n",
    "mnist_competition_file.Label = mnist_competition_file.Label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.to_csv('mnist_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b49f70aa2f17b03439dc8f4bbaf601f728142d0d0d774f4bbd10ea7a16b86ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('wingpuflake_keras': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
